{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install pyspark\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Building the SparkSession and name \n",
    "# it :'pandas to spark'\n",
    "spark = SparkSession.builder.appName(\n",
    "  \"pandas to spark\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [[1, 'Joe', 85000, 1], [2, 'Henry', 80000, 2], [3, 'Sam', 60000, 2], [4, 'Max', 90000, 1], [5, 'Janet', 69000, 1], [6, 'Randy', 85000, 1], [7, 'Will', 70000, 1]]\n",
    "employee = pd.DataFrame(data, columns=['id', 'name', 'salary', 'departmentId']).astype({'id':'Int64', 'name':'object', 'salary':'Int64', 'departmentId':'Int64'})\n",
    "data = [[1, 'IT'], [2, 'Sales']]\n",
    "department = pd.DataFrame(data, columns=['id', 'name']).astype({'id':'Int64', 'name':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------------+\n",
      "| id| name|salary|departmentId|\n",
      "+---+-----+------+------------+\n",
      "|  1|  Joe| 85000|           1|\n",
      "|  2|Henry| 80000|           2|\n",
      "|  3|  Sam| 60000|           2|\n",
      "|  4|  Max| 90000|           1|\n",
      "|  5|Janet| 69000|           1|\n",
      "|  6|Randy| 85000|           1|\n",
      "|  7| Will| 70000|           1|\n",
      "+---+-----+------+------------+\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|   IT|\n",
      "|  2|Sales|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee = spark.createDataFrame(employee)\n",
    "employee.show()\n",
    "\n",
    "department = spark.createDataFrame(department)\n",
    "department.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+\n",
      "|department|employee|salary|\n",
      "+----------+--------+------+\n",
      "|        IT|     Max| 90000|\n",
      "|        IT|     Joe| 85000|\n",
      "|        IT|   Randy| 85000|\n",
      "|        IT|    Will| 70000|\n",
      "|     Sales|   Henry| 80000|\n",
      "|     Sales|     Sam| 60000|\n",
      "+----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy('departmentId').orderBy(desc('salary'))\n",
    "\n",
    "employee \\\n",
    "    .withColumn('dense_rank', dense_rank().over(window)) \\\n",
    "    .where('dense_rank <= 3 and dense_rank >= 1') \\\n",
    "    .withColumnRenamed('name', 'employee') \\\n",
    "    .join(department.withColumnRenamed('id', 'departmentId') \\\n",
    "          .withColumnRenamed('name', 'department'),\n",
    "          'departmentId',\n",
    "          'inner'\n",
    "    ) \\\n",
    "    .select(['department', 'employee', 'salary']) \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
